# Building a local AI server with multiple GPUs using MCP and Ollama


### Hardware Components

Here is a summary of the components used in this build. Each component was chosen to provide a balance of performance, stability, and scalability for AI workloads.

| Component | Model | Quantity |
| :--- | :--- | :--- |
| Motherboard | ASUS Pro WS WRX80E-SAGE SE | 1 |
| CPU | AMD Ryzen Threadripper PRO 5975WX | 1 |
| RAM | 128GB Samsung DDR4-3200 ECC | 4x32GB |
| NVMe SSD | Samsung 990 Pro 1TB | 1 |
| Enterprise SSD | Kingston DC600M 1.92TB | 1 |
| Case | Silverstone SST-RM51 | 1 |
| PSU | FSP CANNON PRO 2500W | 1 | 
| GPU | NVIDIA RTX 5090 32GB | 2 |
| CPU Cooler | Noctua NH-U9 TR4-SP3 | 1 |

![Hardware Overview](./hardware.png "A picture of the server components")

- Montagem do PC software
- Instalação do sistema operativo
- Instalação dos drivers
- Configuração do sistema
- Instalação do porttainer

- Instalação do Ollama
- Configuração do Ollama
- Create MCP server
- 
